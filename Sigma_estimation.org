#+TITLE: Rigorous Mathematical Proof of the Error Propagation Method in S-PLUS Hα-Excess Analysis
#+AUTHOR: Luis A. Gutiérrez Soto et al.
#+LINK: ADS Paper: https://ui.adsabs.harvard.edu/abs/2025arXiv250116530G/abstract
#+LATEX_HEADER: \usepackage{cancel}



* Mathematical Derivation
** Variable Definitions
- \( m \): Slope of the stellar locus in the \((r - i)\) vs. \((r - J0660)\) diagram.
- \( \sigma_s \): RMS of residuals from the linear fit.
- \( \sigma_{(r-J0660)} = \sqrt{\sigma_r^2 + \sigma_{J0660}^2} \): Error of the color \((r - J0660)\).
- \( \sigma_{(r-i)} = \sqrt{\sigma_r^2 + \sigma_i^2} \): Error of the color \((r - i)\).

** Derivation of the Ideal Method from First Principles
*** Definition of Residual
The residual between the observed and fitted colors is:
\[
\text{Residual} = (r - J0660)_{\text{obs}} - (r - J0660)_{\text{fit}}
\]
where the fitted color follows the linear relation:
\[
(r - J0660)_{\text{fit}} = m \cdot (r - i) + b
\]
Substituting \((r - i) = r_{\text{obs}} - i_{\text{obs}}\):
\[
\text{Residual} = (r_{\text{obs}} - J0660_{\text{obs}}) - \left[m \cdot (r_{\text{obs}} - i_{\text{obs}}) + b\right]
\]
Simplifying:
\[
\text{Residual} = (1 - m)r_{\text{obs}} - J0660_{\text{obs}} + m \cdot i_{\text{obs}} - b
\]

*** Error Propagation Formula
The total uncertainty in the residual is:
\[
\sigma_{\text{est}}^2 = \sigma_s^2 + \sum_{x} \left(\frac{\partial \text{Residual}}{\partial x}\right)^2 \sigma_x^2
\]
where \( x = r, i, J0660 \).

*** Partial Derivatives
\[
\begin{aligned}
\frac{\partial \text{Residual}}{\partial r} &= 1 - m \\
\frac{\partial \text{Residual}}{\partial i} &= m \\
\frac{\partial \text{Residual}}{\partial J0660} &= -1 \\
\frac{\partial \text{Residual}}{\partial b} &= -1 \quad (\text{absorbed into } \sigma_s) \\
\end{aligned}
\]

*** Substitute Derivatives into Error Propagation
\[
\sigma_{\text{est}}^2 = \sigma_s^2 + (1 - m)^2 \sigma_r^2 + m^2 \sigma_i^2 + (-1)^2 \sigma_{J0660}^2
\]
\[
\Rightarrow \boxed{\sigma_{\text{est, ideal}}^2 = \sigma_s^2 + (1 - m)^2 \sigma_r^2 + \sigma_{J0660}^2 + m^2 \sigma_i^2}
\]

** Error Propagation Equations
*** Classical Method

Now, we try to writte the equation similar those used in Witham et al. (2206);

\[
\Rightarrow \sigma_{\text{est, ideal}}^2 = \sigma_s^2 + (1 - 2m + m^2) \sigma_r^2 + \sigma_{J0660}^2 + m^2 \sigma_i^2
\]

\[
\Rightarrow \sigma_{\text{est, ideal}}^2 = \sigma_s^2 + \sigma_r^2 - 2m \sigma_r^2 + m^2 \sigma_r^2 + \sigma_{J0660}^2 + m^2 \sigma_i^2
\]

Rearranging terms;

\[
\Rightarrow \sigma_{\text{est, ideal}}^2 = \sigma_s^2 + \sigma_r^2 + \sigma_{J0660}^2 - 2m \sigma_r^2 + m^2 \sigma_r^2  + m^2 \sigma_i^2
\]

Given  \(\sigma_x^2 + \sigma_y^2 = \sigma_{(x - y)}^2 \) then;


\[
\Rightarrow \sigma_{\text{est, ideal}}^2 = \sigma_s^2 + \sigma_{r - J0660}^2 +  m^2 \sigma_{r - i}^2 - 2m \sigma_r^2 
\]

So, to obtain the equation from Witham et al., we have to remove the contribution of the \(- 2m \sigma_r^2\) term. How much does this term contribute?


\[
\sigma_{\text{est, classical}}^2 = \sigma_s^2 + \sigma_{(r-J0660)}^2 + m^2 \sigma_{(r-i)}^2
\]
Go back to the individual error bands:
\[
\sigma_{\text{est, classical}}^2 = \sigma_s^2 + (\sigma_r^2 + \sigma_{J0660}^2) + m^2 (\sigma_r^2 + \sigma_i^2)
\]
\[
= \sigma_s^2 + \sigma_r^2(1 + m^2) + \sigma_{J0660}^2 + m^2 \sigma_i^2 \quad \text{(Overcounts } \sigma_r^2 \text{)}
\]

Now we do some numerical examples. I note to the slope of the fits in the color-color diagram  in most of the case is between 0.35 - 0.4, used in this case 0.4 for the slope:

#+name: error-table
|     e_r_PStotal   |   e_i_PStotal   |  e_J0660_PStotal |
|-------------------+-----------------+------------------|
| 0.0027043421      | 0.002309852     | 0.0034417643     |
| 0.0009554346      | 0.0010019763    | 0.0010847448     |
| 0.0016022029      | 0.0014943366    | 0.0017039843     |
| 0.0022326636      | 0.0020145355    | 0.0023408893     |
| 0.00428348        | 0.0026568426    | 0.004832741      |
| 0.0008637211      | 0.0008142423    | 0.0009804317     |
| 0.0037895583      | 0.0036628896    | 0.0044751875     |
| 0.0026341775      | 0.0025405292    | 0.0020656579     |
| 0.0011333079      | 0.0010817236    | 0.0010882834     |
| 0.0013017068      | 0.0012364228    | 0.0009283392     |
| 0.0032339918      | 0.0032643287    | 0.0035206582     |
| 0.0015321858      | 0.0014926838    | 0.0017911163     |
| 0.0023779713      | 0.0022736047    | 0.0022346096     |
| 0.003833847       | 0.003391762     | 0.0039768526     |
| 0.0032339613      | 0.0026312915    | 0.0031807977     |
| 0.0024073187      | 0.0020921866    | 0.002365463      |
| 0.0029173244      | 0.0024788384    | 0.0036630423     |
| 0.0023025507      | 0.0022479515    | 0.0026816207     |
| 0.0023555679      | 0.0023168672    | 0.002819191      |
| 0.0033654603      | 0.0033328792    | 0.0038815523     |
| 0.0025632957      | 0.0023379133    | 0.0025938326     |
| 0.00349301        | 0.003411095     | 0.003810881      |
| 0.0030717351      | 0.0030049048    | 0.0032032381     |
| 0.001193243       | 0.0011562117    | 0.0009968877     |
| 0.0013465676      | 0.0013025336    | 0.0008975989     |
| 0.0013131873      | 0.0011755121    | 0.00084322726    |
| 0.0013177483      | 0.0012559775    | 0.0008621476     |
| 0.0012351323      | 0.0011085502    | 0.0008751316     |
| 0.0012229811      | 0.0011141312    | 0.00087049545    |
| 0.001374798       | 0.0013328108    | 0.0009173866     |
| 0.0013861168      | 0.001221707     | 0.0008832612     |
| 0.0025332794      | 0.002932436     | 0.0027128041     |
| 0.0036067879      | 0.0031376323    | 0.0037044238     |
| 0.00339293        | 0.0033208467    | 0.004346304      |
| 0.0014183201      | 0.0013238328    | 0.0016154384     |
| 0.004621427       | 0.0047028367    | 0.0046202843     |
| 0.0014658732      | 0.0013899308    | 0.00256149       |
| 0.0023711754      | 0.0022108082    | 0.0026406425     |
| 0.002516076       | 0.0023366914    | 0.002748363      |
| 0.0019471686      | 0.0018115259    | 0.004527632      |
| 0.0033787978      | 0.0030290852    | 0.003503075      |
| 0.0021783817      | 0.0021271813    | 0.0019475339     |
| 0.0036444226      | 0.0033581583    | 0.003897624      |
| 0.002823925       | 0.0026747882    | 0.003112585      |
| 0.0035756512      | 0.0033058396    | 0.00492577       |
| 0.0034714018      | 0.003403292     | 0.0038095748     |
| 0.0029721686      | 0.0027480372    | 0.0029622987     |
| 0.002644481       | 0.0026835275    | 0.0025978298     |
| 0.0028707138      | 0.002735543     | 0.0032248157     |

+ Calculating overestimation
Use this Python block to compute and render the table plus mean/median when exporting to HTML:

#+name: calculate-overestimation
#+begin_src python :var data=error-table :var m=0.4 :var sigma_s=0.01 :results value table
import numpy as np

# Header row
header = ["e_r", "e_i", "e_J0660", "σ_ideal", "σ_classic", "Overestimation (%)"]
new_table = []
overest = []

# Process each data row
for sr_str, si_str, sj_str in data:
    try:
        sr = float(sr_str)
        si = float(si_str)
        sj = float(sj_str)
        σ_ideal   = np.sqrt(sigma_s**2 + (1 - m)**2 * sr**2 + sj**2 + m**2 * si**2)
        σ_classic = np.sqrt(sigma_s**2 + (sr**2 + sj**2) + m**2 * (sr**2 + si**2))
        pct = (σ_classic - σ_ideal) / σ_ideal * 100
        new_table.append([f"{sr:.7f}", f"{si:.7f}", f"{sj:.7f}",
                          f"{σ_ideal:.7f}", f"{σ_classic:.7f}", f"{pct:.2f}%"])
        overest.append(pct)
    except:
        new_table.append([sr_str, si_str, sj_str, "Error", "Error", "Error"])

# Add summary rows
mean_pct   = np.mean(overest)
median_pct = np.median(overest)
new_table.append(["", "", "Mean", "", "", f"{mean_pct:.2f}%"])
new_table.append(["", "", "Median", "", "", f"{median_pct:.2f}%"])

return [header] + new_table
#+end_src

#+RESULTS: calculate-overestimation
|       e_r |       e_i |   e_J0660 |   σ_ideal | σ_classic | Overestimation (%) |
| 0.0027043 | 0.0023099 | 0.0034418 | 0.0107393 | 0.0110083 |              2.51% |
| 0.0009554 | 0.0010020 | 0.0010847 | 0.0100830 | 0.0101191 |              0.36% |
| 0.0016022 | 0.0014943 | 0.0017040 | 0.0102071 | 0.0103072 |              0.98% |
| 0.0022327 | 0.0020145 | 0.0023409 | 0.0103886 | 0.0105788 |              1.83% |
| 0.0042835 | 0.0026568 | 0.0048327 | 0.0114495 | 0.0120735 |              5.45% |
| 0.0008637 | 0.0008142 | 0.0009804 | 0.0100666 | 0.0100962 |              0.29% |
| 0.0037896 | 0.0036629 | 0.0044752 | 0.0112847 | 0.0117827 |              4.41% |
| 0.0026342 | 0.0025405 | 0.0020657 | 0.0103826 | 0.0106465 |              2.54% |
| 0.0011333 | 0.0010817 | 0.0010883 | 0.0100913 | 0.0101421 |              0.50% |
| 0.0013017 | 0.0012364 | 0.0009283 | 0.0100855 | 0.0101524 |              0.66% |
| 0.0032340 | 0.0032643 | 0.0035207 | 0.0108566 | 0.0112353 |              3.49% |
| 0.0015322 | 0.0014927 | 0.0017911 | 0.0102181 | 0.0103096 |              0.90% |
| 0.0023780 | 0.0022736 | 0.0022346 | 0.0103854 | 0.0106009 |              2.08% |
| 0.0038338 | 0.0033918 | 0.0039769 | 0.0110882 | 0.0116063 |              4.67% |
| 0.0032340 | 0.0026313 | 0.0031808 | 0.0107234 | 0.0111066 |              3.57% |
| 0.0024073 | 0.0020922 | 0.0023655 | 0.0104107 | 0.0106310 |              2.12% |
| 0.0029173 | 0.0024788 | 0.0036630 | 0.0108381 | 0.0111478 |              2.86% |
| 0.0023026 | 0.0022480 | 0.0026816 | 0.0104837 | 0.0106841 |              1.91% |
| 0.0023556 | 0.0023169 | 0.0028192 | 0.0105264 | 0.0107351 |              1.98% |
| 0.0033655 | 0.0033329 | 0.0038816 | 0.0109964 | 0.0114010 |              3.68% |
| 0.0025633 | 0.0023379 | 0.0025938 | 0.0104866 | 0.0107343 |              2.36% |
| 0.0034930 | 0.0034111 | 0.0038109 | 0.0109899 | 0.0114253 |              3.96% |
| 0.0030717 | 0.0030049 | 0.0032032 | 0.0107286 | 0.0110748 |              3.23% |
| 0.0011932 | 0.0011562 | 0.0009969 | 0.0100856 | 0.0101420 |              0.56% |
| 0.0013466 | 0.0013025 | 0.0008976 | 0.0100861 | 0.0101578 |              0.71% |
| 0.0013132 | 0.0011755 | 0.0008432 | 0.0100773 | 0.0101456 |              0.68% |
| 0.0013177 | 0.0012560 | 0.0008621 | 0.0100807 | 0.0101494 |              0.68% |
| 0.0012351 | 0.0011086 | 0.0008751 | 0.0100753 | 0.0101357 |              0.60% |
| 0.0012230 | 0.0011141 | 0.0008705 | 0.0100745 | 0.0101337 |              0.59% |
| 0.0013748 | 0.0013328 | 0.0009174 | 0.0100899 | 0.0101646 |              0.74% |
| 0.0013861 | 0.0012217 | 0.0008833 | 0.0100852 | 0.0101611 |              0.75% |
| 0.0025333 | 0.0029324 | 0.0027128 | 0.0105378 | 0.0107787 |              2.29% |
| 0.0036068 | 0.0031376 | 0.0037044 | 0.0109536 | 0.0114188 |              4.25% |
| 0.0033929 | 0.0033208 | 0.0043463 | 0.0111714 | 0.0115762 |              3.62% |
| 0.0014183 | 0.0013238 | 0.0016154 | 0.0101791 | 0.0102579 |              0.77% |
| 0.0046214 | 0.0047028 | 0.0046203 | 0.0115141 | 0.0122336 |              6.25% |
| 0.0014659 | 0.0013899 | 0.0025615 | 0.0103752 | 0.0104577 |              0.80% |
| 0.0023712 | 0.0022108 | 0.0026406 | 0.0104776 | 0.0106900 |              2.03% |
| 0.0025161 | 0.0023367 | 0.0027484 | 0.0105217 | 0.0107597 |              2.26% |
| 0.0019472 | 0.0018115 | 0.0045276 | 0.0110630 | 0.0111992 |              1.23% |
| 0.0033788 | 0.0030291 | 0.0035031 | 0.0108558 | 0.0112686 |              3.80% |
| 0.0021784 | 0.0021272 | 0.0019475 | 0.0103066 | 0.0104891 |              1.77% |
| 0.0036444 | 0.0033582 | 0.0038976 | 0.0110353 | 0.0115066 |              4.27% |
| 0.0028239 | 0.0026748 | 0.0031126 | 0.0106632 | 0.0109583 |              2.77% |
| 0.0035757 | 0.0033058 | 0.0049258 | 0.0114287 | 0.0118677 |              3.84% |
| 0.0034714 | 0.0034033 | 0.0038096 | 0.0109865 | 0.0114169 |              3.92% |
| 0.0029722 | 0.0027480 | 0.0029623 | 0.0106378 | 0.0109650 |              3.08% |
| 0.0026445 | 0.0026835 | 0.0025978 | 0.0105080 | 0.0107709 |              2.50% |
| 0.0028707 | 0.0027355 | 0.0032248 | 0.0107034 | 0.0110071 |              2.84% |
|           |           |      Mean |           |           |              2.33% |
|           |           |    Median |           |           |              2.26% |

The classical estimation of σσ results in an overestimation of about 2.32%, considering m=0.4,
which is typical for the fit in the stellar locus color–color diagram of S-PLUS, and assuming sigma_ss=0.01.

*** Luis's Method (Proposed)

See if I get a better apprximation on wich the covariance term contribute lees to the sigma value. Go back again to the ideal case:

\[
\Rightarrow \sigma_{\text{est, ideal}}^2 = \sigma_s^2 + \sigma_r^2 + \sigma_{J0660}^2 - 2m \sigma_r^2 + m^2 \sigma_r^2  + m^2 \sigma_i^2
\]

Rearrange;

\[
\Rightarrow \boxed{\sigma_{\text{est, ideal}}^2 = \sigma_s^2 + \sigma_r^2 + \sigma_{J0660}^2 + m^2 (\sigma_r^2  + \sigma_i^2}) - 2m \sigma_r^2
\]

Add and subtract the term: \(-2m(\sigma_r^2 + \sigma_{J0660}^2) + m^2(\sigma_r^2 + \sigma_{J0660}^2\), then:

\[
\Rightarrow \sigma_{\text{est, ideal}}^2 = \sigma_s^2 + \sigma_r^2 + \sigma_{J0660}^2 + m^2 (\sigma_r^2  + \sigma_i^2) - 2m \sigma_r^2 + (-2m(\sigma_r^2 + \sigma_{J0660}^2)+ m^2(\sigma_r^2 + \sigma_{J0660}^2)) - (-2m(\sigma_r^2 + \sigma_{J0660}^2)+ m^2(\sigma_r^2 + \sigma_{J0660}^2))
\]

Next, we rearrange and factorize:

\[
\Rightarrow \sigma_{\text{est, ideal}}^2 = \sigma_s^2 + (1 - 2m + m^2)(\sigma_r^2 + \sigma_{J0660}^2) + m^2 (\sigma_r^2  + \sigma_i^2) - 2m \sigma_r^2 - (-2m(\sigma_r^2 + \sigma_{J0660}^2) + m^2(\sigma_r^2 + \sigma_{J0660}^2))
\]

\[
\Rightarrow \boxed{\sigma_{\text{est, ideal}}^2 = \sigma_s^2 + (1 - m)^2(\sigma_r^2 + \sigma_{J0660}^2) + m^2 (\sigma_r^2  + \sigma_i^2}) + 2m\sigma_{J0660}^2 - m^2 \sigma_r^2 - m^2 \sigma_{J0660}^2
\]

\( \Rightarrow \sigma_{\mathrm{est,ideal}}^2 = \sigma_s^2 + (1 - m)^2(\sigma_r^2 + \sigma_{J0660}^2) + m^2(\sigma_r^2 + \sigma_i^2) - m^2\sigma_r^2 - m^2\sigma_{J0660}^2 + 2m\,\sigma_{J0660}^2 \)

\( \Rightarrow \sigma_{\mathrm{est,ideal}}^2 = \sigma_s^2 + (1 - m)^2(\sigma_r^2 + \sigma_{J0660}^2) + m^2(\sigma_r^2 + \sigma_i^2) - m^2\sigma_r^2 + (2m - m^2)\,\sigma_{J0660}^2 \)


The covariance term is: \(-m^2 \sigma_r^2 + (2m - m^2) \sigma_{J0660}^2\). Therefore, **the contribution** of this term is:

*** Simplified error estimate
By removing the covariance term, we keep only the color errors:

\( \sigma_{\mathrm{est,Luis}}^2 = \sigma_s^2 + (1 - m)^2\,\sigma_{(r - J0660)}^2 + m^2\,\sigma_{(r - i)}^2 \)

Contribution of the Removed Covariance Term


#+name: calculate-overestimation-Luis
#+begin_src python :var data=error-table :var m=0.4 :var sigma_s=0.01 :results output raw
import numpy as np

# Encabezado de la tabla
print("| e_r | e_i | e_J0660 | σ_ideal | σ_luis | Overestimation (%) |")
print("|-")

overest = []

for sr_str, si_str, sj_str in data:
    try:
        sr = float(sr_str); si = float(si_str); sj = float(sj_str)
        
        σ_ideal = np.sqrt(sigma_s**2 + (1 - m)**2 * sr**2 + sj**2 + m**2 * si**2)
        σ_luis = np.sqrt(sigma_s**2 + (1 - m)**2 * (sr**2 + sj**2) + m**2 * (sr**2 + si**2))
        
        pct = (σ_luis - σ_ideal) / σ_ideal * 100
        overest.append(pct)
        
        print(f"| {sr:.7f} | {si:.7f} | {sj:.7f} | {σ_ideal:.7f} | {σ_luis:.7f} | {pct:.2f}% |")
    except:
        print(f"| Error | Error | Error | Error | Error | Error |")

# Estadísticas finales
mean_pct = np.mean(overest)
median_pct = np.median(overest)
print("|-")
print(f"| | | Mean | | | {mean_pct:.2f}% |")
print(f"| | | Median | | | {median_pct:.2f}% |")
#+end_src

#+RESULTS: calculate-overestimation-Luis
|       e_r |       e_i |   e_J0660 |   σ_ideal |    σ_luis | Overestimation (%) |
|-----------+-----------+-----------+-----------+-----------+--------------------|
| 0.0027043 | 0.0023099 | 0.0034418 | 0.0107393 | 0.0104365 |             -2.82% |
| 0.0009554 | 0.0010020 | 0.0010847 | 0.0100830 | 0.0100528 |             -0.30% |
| 0.0016022 | 0.0014943 | 0.0017040 | 0.0102071 | 0.0101359 |             -0.70% |
| 0.0022327 | 0.0020145 | 0.0023409 | 0.0103886 | 0.0102574 |             -1.26% |
| 0.0042835 | 0.0026568 | 0.0048327 | 0.0114495 | 0.0109123 |             -4.69% |
| 0.0008637 | 0.0008142 | 0.0009804 | 0.0100666 | 0.0100419 |             -0.24% |
| 0.0037896 | 0.0036629 | 0.0044752 | 0.0112847 | 0.0108085 |             -4.22% |
| 0.0026342 | 0.0025405 | 0.0020657 | 0.0103826 | 0.0103042 |             -0.75% |
| 0.0011333 | 0.0010817 | 0.0010883 | 0.0100913 | 0.0100639 |             -0.27% |
| 0.0013017 | 0.0012364 | 0.0009283 | 0.0100855 | 0.0100715 |             -0.14% |
| 0.0032340 | 0.0032643 | 0.0035207 | 0.0108566 | 0.0105644 |             -2.69% |
| 0.0015322 | 0.0014927 | 0.0017911 | 0.0102181 | 0.0101357 |             -0.81% |
| 0.0023780 | 0.0022736 | 0.0022346 | 0.0103854 | 0.0102745 |             -1.07% |
| 0.0038338 | 0.0033918 | 0.0039769 | 0.0110882 | 0.0107321 |             -3.21% |
| 0.0032340 | 0.0026313 | 0.0031808 | 0.0107234 | 0.0104971 |             -2.11% |
| 0.0024073 | 0.0020922 | 0.0023655 | 0.0104107 | 0.0102824 |             -1.23% |
| 0.0029173 | 0.0024788 | 0.0036630 | 0.0108381 | 0.0104995 |             -3.12% |
| 0.0023026 | 0.0022480 | 0.0026816 | 0.0104837 | 0.0103031 |             -1.72% |
| 0.0023556 | 0.0023169 | 0.0028192 | 0.0105264 | 0.0103250 |             -1.91% |
| 0.0033655 | 0.0033329 | 0.0038816 | 0.0109964 | 0.0106344 |             -3.29% |
| 0.0025633 | 0.0023379 | 0.0025938 | 0.0104866 | 0.0103302 |             -1.49% |
| 0.0034930 | 0.0034111 | 0.0038109 | 0.0109899 | 0.0106506 |             -3.09% |
| 0.0030717 | 0.0030049 | 0.0032032 | 0.0107286 | 0.0104902 |             -2.22% |
| 0.0011932 | 0.0011562 | 0.0009969 | 0.0100856 | 0.0100654 |             -0.20% |
| 0.0013466 | 0.0013025 | 0.0008976 | 0.0100861 | 0.0100749 |             -0.11% |
| 0.0013132 | 0.0011755 | 0.0008432 | 0.0100773 | 0.0100685 |             -0.09% |
| 0.0013177 | 0.0012560 | 0.0008621 | 0.0100807 | 0.0100709 |             -0.10% |
| 0.0012351 | 0.0011086 | 0.0008751 | 0.0100753 | 0.0100631 |             -0.12% |
| 0.0012230 | 0.0011141 | 0.0008705 | 0.0100745 | 0.0100623 |             -0.12% |
| 0.0013748 | 0.0013328 | 0.0009174 | 0.0100899 | 0.0100782 |             -0.12% |
| 0.0013861 | 0.0012217 | 0.0008833 | 0.0100852 | 0.0100757 |             -0.09% |
| 0.0025333 | 0.0029324 | 0.0027128 | 0.0105378 | 0.0103616 |             -1.67% |
| 0.0036068 | 0.0031376 | 0.0037044 | 0.0109536 | 0.0106433 |             -2.83% |
| 0.0033929 | 0.0033208 | 0.0043463 | 0.0111714 | 0.0107029 |             -4.19% |
| 0.0014183 | 0.0013238 | 0.0016154 | 0.0101791 | 0.0101127 |             -0.65% |
| 0.0046214 | 0.0047028 | 0.0046203 | 0.0115141 | 0.0110603 |             -3.94% |
| 0.0014659 | 0.0013899 | 0.0025615 | 0.0103752 | 0.0101877 |             -1.81% |
| 0.0023712 | 0.0022108 | 0.0026406 | 0.0104776 | 0.0103061 |             -1.64% |
| 0.0025161 | 0.0023367 | 0.0027484 | 0.0105217 | 0.0103385 |             -1.74% |
| 0.0019472 | 0.0018115 | 0.0045276 | 0.0110630 | 0.0104822 |             -5.25% |
| 0.0033788 | 0.0030291 | 0.0035031 | 0.0108558 | 0.0105746 |             -2.59% |
| 0.0021784 | 0.0021272 | 0.0019475 | 0.0103066 | 0.0102253 |             -0.79% |
| 0.0036444 | 0.0033582 | 0.0038976 | 0.0110353 | 0.0106855 |             -3.17% |
| 0.0028239 | 0.0026748 | 0.0031126 | 0.0106632 | 0.0104297 |             -2.19% |
| 0.0035757 | 0.0033058 | 0.0049258 | 0.0114287 | 0.0108227 |             -5.30% |
| 0.0034714 | 0.0034033 | 0.0038096 | 0.0109865 | 0.0106463 |             -3.10% |
| 0.0029722 | 0.0027480 | 0.0029623 | 0.0106378 | 0.0104384 |             -1.87% |
| 0.0026445 | 0.0026835 | 0.0025978 | 0.0105080 | 0.0103546 |             -1.46% |
| 0.0028707 | 0.0027355 | 0.0032248 | 0.0107034 | 0.0104511 |             -2.36% |
|-----------+-----------+-----------+-----------+-----------+--------------------|
|           |           |      Mean |           |           |             -1.85% |
|           |           |    Median |           |           |             -1.72% |

The luis aproximation subestima the sigma in around -1.85%.

**** Classic versus Luis aproximation
Now Comparing Classic vs. Luis Overestimation

#+name: calculate-overestimation-compare
#+begin_src python :var data=error-table :var m=0.4 :var sigma_s=0.01 :return table 
import numpy as np

# Header row
header = [
    "e_r", "e_i", "e_J0660",
    "σ_ideal", "σ_classic", "σ_luis",
    "Classic overestimation (%)", "Luis overestimation (%)"
]

new_table = []
overest_classic = []
overest_luis = []

# Process each data row
for sr_str, si_str, sj_str in data:
    try:
        sr = float(sr_str)
        si = float(si_str)
        sj = float(sj_str)

        σ_ideal   = np.sqrt(
            sigma_s**2 +
            (1 - m)**2 * sr**2 +
            sj**2 +
            m**2 * si**2
        )
        σ_classic = np.sqrt(
            sigma_s**2 +
            (sr**2 + sj**2) +
            m**2 * (sr**2 + si**2)
        )
        σ_luis    = np.sqrt(
            sigma_s**2 +
            (1 - m)**2 * (sr**2 + sj**2) +
            m**2 * (sr**2 + si**2)
        )

        pct_classic = (σ_classic - σ_ideal) / σ_ideal * 100
        pct_luis    = (σ_luis    - σ_ideal) / σ_ideal * 100

        new_table.append([
            f"{sr:.7f}", f"{si:.7f}", f"{sj:.7f}",
            f"{σ_ideal:.7f}", f"{σ_classic:.7f}", f"{σ_luis:.7f}",
            f"{pct_classic:.2f}%", f"{pct_luis:.2f}%"
        ])
        overest_classic.append(pct_classic)
        overest_luis.append(pct_luis)

    except:
        new_table.append([sr_str, si_str, sj_str] + ["Error"] * 5)

# Compute summary statistics
mean_c   = np.mean(overest_classic)
median_c = np.median(overest_classic)
min_c    = np.min(overest_classic)
max_c    = np.max(overest_classic)

mean_l   = np.mean(overest_luis)
median_l = np.median(overest_luis)
min_l    = np.min(overest_luis)
max_l    = np.max(overest_luis)

# Append summary rows
new_table.append(["", "", "Mean", "", "", "",
                  f"{mean_c:.2f}%", f"{mean_l:.2f}%"])
new_table.append(["", "", "Median", "", "", "",
                  f"{median_c:.2f}%", f"{median_l:.2f}%"])
new_table.append(["", "", "Min", "", "", "",
                  f"{min_c:.2f}%", f"{min_l:.2f}%"])
new_table.append(["", "", "Max", "", "", "",
                  f"{max_c:.2f}%", f"{max_l:.2f}%"])

# Return full table
return [header] + new_table
#+end_src


#+RESULTS: calculate-overestimation-compare
|       e_r |       e_i |   e_J0660 |   σ_ideal | σ_classic |    σ_luis | Classic overestimation (%) | Luis overestimation (%) |
| 0.0027043 | 0.0023099 | 0.0034418 | 0.0107393 | 0.0110083 | 0.0104365 |                      2.51% |                  -2.82% |
| 0.0009554 | 0.0010020 | 0.0010847 | 0.0100830 | 0.0101191 | 0.0100528 |                      0.36% |                  -0.30% |
| 0.0016022 | 0.0014943 | 0.0017040 | 0.0102071 | 0.0103072 | 0.0101359 |                      0.98% |                  -0.70% |
| 0.0022327 | 0.0020145 | 0.0023409 | 0.0103886 | 0.0105788 | 0.0102574 |                      1.83% |                  -1.26% |
| 0.0042835 | 0.0026568 | 0.0048327 | 0.0114495 | 0.0120735 | 0.0109123 |                      5.45% |                  -4.69% |
| 0.0008637 | 0.0008142 | 0.0009804 | 0.0100666 | 0.0100962 | 0.0100419 |                      0.29% |                  -0.24% |
| 0.0037896 | 0.0036629 | 0.0044752 | 0.0112847 | 0.0117827 | 0.0108085 |                      4.41% |                  -4.22% |
| 0.0026342 | 0.0025405 | 0.0020657 | 0.0103826 | 0.0106465 | 0.0103042 |                      2.54% |                  -0.75% |
| 0.0011333 | 0.0010817 | 0.0010883 | 0.0100913 | 0.0101421 | 0.0100639 |                      0.50% |                  -0.27% |
| 0.0013017 | 0.0012364 | 0.0009283 | 0.0100855 | 0.0101524 | 0.0100715 |                      0.66% |                  -0.14% |
| 0.0032340 | 0.0032643 | 0.0035207 | 0.0108566 | 0.0112353 | 0.0105644 |                      3.49% |                  -2.69% |
| 0.0015322 | 0.0014927 | 0.0017911 | 0.0102181 | 0.0103096 | 0.0101357 |                      0.90% |                  -0.81% |
| 0.0023780 | 0.0022736 | 0.0022346 | 0.0103854 | 0.0106009 | 0.0102745 |                      2.08% |                  -1.07% |
| 0.0038338 | 0.0033918 | 0.0039769 | 0.0110882 | 0.0116063 | 0.0107321 |                      4.67% |                  -3.21% |
| 0.0032340 | 0.0026313 | 0.0031808 | 0.0107234 | 0.0111066 | 0.0104971 |                      3.57% |                  -2.11% |
| 0.0024073 | 0.0020922 | 0.0023655 | 0.0104107 | 0.0106310 | 0.0102824 |                      2.12% |                  -1.23% |
| 0.0029173 | 0.0024788 | 0.0036630 | 0.0108381 | 0.0111478 | 0.0104995 |                      2.86% |                  -3.12% |
| 0.0023026 | 0.0022480 | 0.0026816 | 0.0104837 | 0.0106841 | 0.0103031 |                      1.91% |                  -1.72% |
| 0.0023556 | 0.0023169 | 0.0028192 | 0.0105264 | 0.0107351 | 0.0103250 |                      1.98% |                  -1.91% |
| 0.0033655 | 0.0033329 | 0.0038816 | 0.0109964 | 0.0114010 | 0.0106344 |                      3.68% |                  -3.29% |
| 0.0025633 | 0.0023379 | 0.0025938 | 0.0104866 | 0.0107343 | 0.0103302 |                      2.36% |                  -1.49% |
| 0.0034930 | 0.0034111 | 0.0038109 | 0.0109899 | 0.0114253 | 0.0106506 |                      3.96% |                  -3.09% |
| 0.0030717 | 0.0030049 | 0.0032032 | 0.0107286 | 0.0110748 | 0.0104902 |                      3.23% |                  -2.22% |
| 0.0011932 | 0.0011562 | 0.0009969 | 0.0100856 | 0.0101420 | 0.0100654 |                      0.56% |                  -0.20% |
| 0.0013466 | 0.0013025 | 0.0008976 | 0.0100861 | 0.0101578 | 0.0100749 |                      0.71% |                  -0.11% |
| 0.0013132 | 0.0011755 | 0.0008432 | 0.0100773 | 0.0101456 | 0.0100685 |                      0.68% |                  -0.09% |
| 0.0013177 | 0.0012560 | 0.0008621 | 0.0100807 | 0.0101494 | 0.0100709 |                      0.68% |                  -0.10% |
| 0.0012351 | 0.0011086 | 0.0008751 | 0.0100753 | 0.0101357 | 0.0100631 |                      0.60% |                  -0.12% |
| 0.0012230 | 0.0011141 | 0.0008705 | 0.0100745 | 0.0101337 | 0.0100623 |                      0.59% |                  -0.12% |
| 0.0013748 | 0.0013328 | 0.0009174 | 0.0100899 | 0.0101646 | 0.0100782 |                      0.74% |                  -0.12% |
| 0.0013861 | 0.0012217 | 0.0008833 | 0.0100852 | 0.0101611 | 0.0100757 |                      0.75% |                  -0.09% |
| 0.0025333 | 0.0029324 | 0.0027128 | 0.0105378 | 0.0107787 | 0.0103616 |                      2.29% |                  -1.67% |
| 0.0036068 | 0.0031376 | 0.0037044 | 0.0109536 | 0.0114188 | 0.0106433 |                      4.25% |                  -2.83% |
| 0.0033929 | 0.0033208 | 0.0043463 | 0.0111714 | 0.0115762 | 0.0107029 |                      3.62% |                  -4.19% |
| 0.0014183 | 0.0013238 | 0.0016154 | 0.0101791 | 0.0102579 | 0.0101127 |                      0.77% |                  -0.65% |
| 0.0046214 | 0.0047028 | 0.0046203 | 0.0115141 | 0.0122336 | 0.0110603 |                      6.25% |                  -3.94% |
| 0.0014659 | 0.0013899 | 0.0025615 | 0.0103752 | 0.0104577 | 0.0101877 |                      0.80% |                  -1.81% |
| 0.0023712 | 0.0022108 | 0.0026406 | 0.0104776 | 0.0106900 | 0.0103061 |                      2.03% |                  -1.64% |
| 0.0025161 | 0.0023367 | 0.0027484 | 0.0105217 | 0.0107597 | 0.0103385 |                      2.26% |                  -1.74% |
| 0.0019472 | 0.0018115 | 0.0045276 | 0.0110630 | 0.0111992 | 0.0104822 |                      1.23% |                  -5.25% |
| 0.0033788 | 0.0030291 | 0.0035031 | 0.0108558 | 0.0112686 | 0.0105746 |                      3.80% |                  -2.59% |
| 0.0021784 | 0.0021272 | 0.0019475 | 0.0103066 | 0.0104891 | 0.0102253 |                      1.77% |                  -0.79% |
| 0.0036444 | 0.0033582 | 0.0038976 | 0.0110353 | 0.0115066 | 0.0106855 |                      4.27% |                  -3.17% |
| 0.0028239 | 0.0026748 | 0.0031126 | 0.0106632 | 0.0109583 | 0.0104297 |                      2.77% |                  -2.19% |
| 0.0035757 | 0.0033058 | 0.0049258 | 0.0114287 | 0.0118677 | 0.0108227 |                      3.84% |                  -5.30% |
| 0.0034714 | 0.0034033 | 0.0038096 | 0.0109865 | 0.0114169 | 0.0106463 |                      3.92% |                  -3.10% |
| 0.0029722 | 0.0027480 | 0.0029623 | 0.0106378 | 0.0109650 | 0.0104384 |                      3.08% |                  -1.87% |
| 0.0026445 | 0.0026835 | 0.0025978 | 0.0105080 | 0.0107709 | 0.0103546 |                      2.50% |                  -1.46% |
| 0.0028707 | 0.0027355 | 0.0032248 | 0.0107034 | 0.0110071 | 0.0104511 |                      2.84% |                  -2.36% |
|           |           |      Mean |           |           |           |                      2.33% |                  -1.85% |
|           |           |    Median |           |           |           |                      2.26% |                  -1.72% |
|           |           |       Min |           |           |           |                      0.29% |                  -5.30% |
|           |           |       Max |           |           |           |                      6.25% |                  -0.09% |

Clearly, Luis’s approximation is closer to the ideal case, underestimating by only about 1.85%, compared to the classical method, which overestimates by approximately 2.33%.

Now estimate the value of the covariance term for both case:

#+name: calculate-covarianze
#+begin_src python :var data=error-table :var m=0.4 :var sigma_s=0.01 :results value table
import numpy as np

# Header for the output table
header = ["e_r", "e_i", "e_J0660", "Cov_classic", "Cov_Luis"]
output = []

# Compute covariance terms for each row
for sr_str, si_str, sj_str in data:
    try:
        sr = float(sr_str)
        sj = float(sj_str)
        # Classical covariance term
        cov_classic = -2 * m * sr**2
        # Luis covariance term
        cov_luis = -m**2 * sr**2 + (2 * m - m**2) * sj**2

        output.append([
            f"{sr:.7f}",
            si_str,
            f"{sj:.7f}",
            f"{cov_classic:.7e}",
            f"{cov_luis:.7e}"
        ])
    except Exception:
        output.append([sr_str, si_str, sj_str, "Error", "Error"])

# Return header + computed rows
return [header] + output
#+end_src

#+RESULTS: calculate-covarianze
|       e_r |          e_i |   e_J0660 |    Cov_classic |      Cov_Luis |
| 0.0027043 |  0.002309852 | 0.0034418 | -5.8507730e-06 | 6.4111200e-06 |
| 0.0009554 | 0.0010019763 | 0.0010847 | -7.3028422e-07 | 6.0701278e-07 |
| 0.0016022 | 0.0014943366 | 0.0017040 | -2.0536433e-06 | 1.4475513e-06 |
| 0.0022327 | 0.0020145355 | 0.0023409 | -3.9878294e-06 | 2.7094823e-06 |
| 0.0042835 | 0.0026568426 | 0.0048327 | -1.4678561e-05 | 1.2011735e-05 |
| 0.0008637 | 0.0008142423 | 0.0009804 | -5.9681131e-07 | 4.9583538e-07 |
| 0.0037896 | 0.0036628896 | 0.0044752 | -1.1488602e-05 | 1.0519754e-05 |
| 0.0026342 | 0.0025405292 | 0.0020657 | -5.5511129e-06 | 1.6206207e-06 |
| 0.0011333 | 0.0010817236 | 0.0010883 | -1.0275094e-06 | 5.5248900e-07 |
| 0.0013017 | 0.0012364228 | 0.0009283 | -1.3555525e-06 | 2.8045025e-07 |
| 0.0032340 | 0.0032643287 | 0.0035207 | -8.3669624e-06 | 6.2594294e-06 |
| 0.0015322 | 0.0014926838 | 0.0017911 | -1.8780747e-06 | 1.6775675e-06 |
| 0.0023780 | 0.0022736047 | 0.0022346 | -4.5237980e-06 | 2.2910676e-06 |
| 0.0038338 |  0.003391762 | 0.0039769 | -1.1758706e-05 | 7.7700870e-06 |
| 0.0032340 | 0.0026312915 | 0.0031808 | -8.3668046e-06 | 4.8018225e-06 |
| 0.0024073 | 0.0020921866 | 0.0023655 | -4.6361467e-06 | 2.6538364e-06 |
| 0.0029173 | 0.0024788384 | 0.0036630 | -6.8086253e-06 | 7.2257174e-06 |
| 0.0023026 | 0.0022479515 | 0.0026816 | -4.2413918e-06 | 3.7540190e-06 |
| 0.0023556 | 0.0023168672 | 0.0028192 | -4.4389601e-06 | 4.1988242e-06 |
| 0.0033655 | 0.0033328792 | 0.0038816 | -9.0610584e-06 | 7.8303152e-06 |
| 0.0025633 | 0.0023379133 | 0.0025938 | -5.2563879e-06 | 3.2546217e-06 |
| 0.0034930 |  0.003411095 | 0.0038109 | -9.7608951e-06 | 7.3424219e-06 |
| 0.0030717 | 0.0030049048 | 0.0032032 | -7.5484452e-06 | 5.0571809e-06 |
| 0.0011932 | 0.0011562117 | 0.0009969 | -1.1390631e-06 | 4.0820984e-07 |
| 0.0013466 | 0.0013025336 | 0.0008976 | -1.4505954e-06 | 2.2551853e-07 |
| 0.0013132 | 0.0011755121 | 0.0008432 | -1.3795687e-06 | 1.7914687e-07 |
| 0.0013177 | 0.0012559775 | 0.0008621 | -1.3891685e-06 | 1.9787734e-07 |
| 0.0012351 | 0.0011085502 | 0.0008751 | -1.2204414e-06 | 2.4605912e-07 |
| 0.0012230 | 0.0011141312 | 0.0008705 | -1.1965462e-06 | 2.4565865e-07 |
| 0.0013748 | 0.0013328108 | 0.0009174 | -1.5120556e-06 | 2.3621170e-07 |
| 0.0013861 |  0.001221707 | 0.0008833 | -1.5370558e-06 | 1.9188506e-07 |
| 0.0025333 |  0.002932436 | 0.0027128 | -5.1340036e-06 | 3.6831552e-06 |
| 0.0036068 | 0.0031376323 | 0.0037044 | -1.0407135e-05 | 6.7011366e-06 |
| 0.0033929 | 0.0033208467 | 0.0043463 | -9.2095792e-06 | 1.0247914e-05 |
| 0.0014183 | 0.0013238328 | 0.0016154 | -1.6093055e-06 | 1.3483093e-06 |
| 0.0046214 | 0.0047028367 | 0.0046203 | -1.7086070e-05 | 1.0244883e-05 |
| 0.0014659 | 0.0013899308 | 0.0025615 | -1.7190274e-06 | 3.8553824e-06 |
| 0.0023712 | 0.0022108082 | 0.0026406 | -4.4979782e-06 | 3.5631198e-06 |
| 0.0025161 | 0.0023366914 | 0.0027484 | -5.0645108e-06 | 3.8213373e-06 |
| 0.0019472 | 0.0018115259 | 0.0045276 | -3.0331724e-06 | 1.2513014e-05 |
| 0.0033788 | 0.0030290852 | 0.0035031 | -9.1330197e-06 | 6.0271781e-06 |
| 0.0021784 | 0.0021271813 | 0.0019475 | -3.7962775e-06 | 1.6681930e-06 |
| 0.0036444 | 0.0033581583 | 0.0038976 | -1.0625453e-05 | 7.5974520e-06 |
| 0.0028239 | 0.0026747882 | 0.0031126 | -6.3796419e-06 | 4.9245103e-06 |
| 0.0035757 | 0.0033058396 | 0.0049258 | -1.0228225e-05 | 1.3482809e-05 |
| 0.0034714 |  0.003403292 | 0.0038096 | -9.6405044e-06 | 7.3601296e-06 |
| 0.0029722 | 0.0027480372 | 0.0029623 | -7.0670289e-06 | 4.2027309e-06 |
| 0.0026445 | 0.0026835275 | 0.0025978 | -5.5946238e-06 | 3.2002558e-06 |
| 0.0028707 |  0.002735543 | 0.0032248 | -6.5927982e-06 | 5.3370796e-06 |

The comparison between the classical covariance and the alternative method implemented by
Luis reveals that the latter consistently yields values closer to zero. This indicates
a weaker linear dependency between the photometric errors of the \(r\), \(i\), and \(J0660\) filters.
A covariance closer to zero suggests that the method proposed by Luis more effectively minimizes
spurious correlations among uncertainties, bringing the error structure closer to the ideal
assumption of independence. Therefore, the "Cov_Luis" approach provides
a more reliable representation of the photometric error relationships in this context.
