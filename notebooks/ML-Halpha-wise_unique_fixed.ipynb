{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Halpha Emitters S-PLUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from astropy.table import Table\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import hdbscan\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "import umap.umap_ as umap\n",
    "import seaborn as sns; sns.set()\n",
    "sns.set_theme(style=\"ticks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.read_csv(\"../Ha-emitters/Halpha_Mine_PerField_total-unique_wise.csv\")\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the distance of the SPLUS nad WISE source\n",
    "# Plotting the histogram\n",
    "plt.hist(combined_df[\"angDist\"], bins=100, color='skyblue', edgecolor='black')  # You can adjust the number of bins as needed\n",
    "plt.title('Histogram of Sample Data')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)  # Add grid lines for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for columns in combined_df.columns:\n",
    "    print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract magnitude errors for WISE filters W1, W2, W3, W4\n",
    "magnitude_errors_w1 = combined_df[\"e_W1mag\"]\n",
    "magnitude_errors_w2 = combined_df[\"e_W2mag\"]\n",
    "magnitude_errors_w3 = combined_df[\"e_W3mag\"]\n",
    "magnitude_errors_w4 = combined_df[\"e_W4mag\"]\n",
    "\n",
    "# Create subplots for each histogram\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Plot histogram for e_W1mag\n",
    "axs[0, 0].hist(magnitude_errors_w1, bins=50, color='skyblue', edgecolor='black')\n",
    "axs[0, 0].set_title('Distribution of Magnitude Errors for WISE Filter W1')\n",
    "axs[0, 0].set_xlabel('Magnitude Error (e_W1mag)')\n",
    "axs[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Plot histogram for e_W2mag\n",
    "axs[0, 1].hist(magnitude_errors_w2, bins=50, color='skyblue', edgecolor='black')\n",
    "axs[0, 1].set_title('Distribution of Magnitude Errors for WISE Filter W2')\n",
    "axs[0, 1].set_xlabel('Magnitude Error (e_W2mag)')\n",
    "axs[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Plot histogram for e_W3mag\n",
    "axs[1, 0].hist(magnitude_errors_w3, bins=50, color='skyblue', edgecolor='black')\n",
    "axs[1, 0].set_title('Distribution of Magnitude Errors for WISE Filter W3')\n",
    "axs[1, 0].set_xlabel('Magnitude Error (e_W3mag)')\n",
    "axs[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Plot histogram for e_W4mag\n",
    "axs[1, 1].hist(magnitude_errors_w4, bins=50, color='skyblue', edgecolor='black')\n",
    "axs[1, 1].set_title('Distribution of Magnitude Errors for WISE Filter W4')\n",
    "axs[1, 1].set_xlabel('Magnitude Error (e_W4mag)')\n",
    "axs[1, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaned error\n",
    "\n",
    "m_err_splus = (combined_df[\"e_r_PStotal\"] <= 0.2) & (combined_df[\"e_g_PStotal\"] <= 0.2) & \\\n",
    "        (combined_df[\"e_i_PStotal\"] <= 0.2) & (combined_df[\"e_u_PStotal\"] <= 0.2) & \\\n",
    "        (combined_df[\"e_J0378_PStotal\"] <= 0.2) & (combined_df[\"e_J0395_PStotal\"] <= 0.2) & \\\n",
    "        (combined_df[\"e_J0410_PStotal\"] <= 0.2) & (combined_df[\"e_J0430_PStotal\"] <= 0.2) & \\\n",
    "        (combined_df[\"e_J0515_PStotal\"] <= 0.2) & (combined_df[\"e_J0660_PStotal\"] <= 0.2) & \\\n",
    "        (combined_df[\"e_J0861_PStotal\"] <= 0.2) & (combined_df[\"e_z_PStotal\"] <= 0.2)\n",
    "\n",
    "# Choose a threshold for the maximum allowed magnitude error\n",
    "max_allowed_e_Wmag = 0.5  # Example threshold value\n",
    "\n",
    "# Apply the threshold to filter the dataset\n",
    "\n",
    "m_err_wise = (combined_df[\"e_W1mag\"] <= max_allowed_e_Wmag) & \\\n",
    "              (combined_df[\"e_W2mag\"] <= max_allowed_e_Wmag) \n",
    "        \n",
    "\n",
    "mask_total = (m_err_splus & m_err_wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleanErr = combined_df[mask_total]\n",
    "len(df_cleanErr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting columns\n",
    "columns = [\"r_PStotal\",\n",
    "\"g_PStotal\",\n",
    "\"i_PStotal\",\n",
    "\"u_PStotal\",\n",
    "\"z_PStotal\",\n",
    "\"J0378_PStotal\",\n",
    "\"J0395_PStotal\",\n",
    "\"J0410_PStotal\",\n",
    "\"J0430_PStotal\",\n",
    "\"J0515_PStotal\",\n",
    "\"J0660_PStotal\",\n",
    "\"J0861_PStotal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mag = df_cleanErr[columns]\n",
    "df_mag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all combinations of magnitude columns\n",
    "color_index_pairs = list(combinations(df_mag, 2))\n",
    "len(color_index_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_earnings(df, index_pairs):\n",
    "    for index_pair in index_pairs:\n",
    "        color_index_name = f\"{index_pair[0]} - {index_pair[1]}\"\n",
    "        df.loc[:, color_index_name] = df[index_pair[0]] - df[index_pair[1]]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_colors_mag = calculate_earnings(df_mag, color_index_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_colors_mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop magnitudes\n",
    "df_colors = df_colors_mag.drop(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the color using some WISE filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate differences between W1 and each magnitude\n",
    "for col in [\"r_PStotal\", \"g_PStotal\", \"i_PStotal\", \"u_PStotal\", \"z_PStotal\"]:\n",
    "    df_colors[f'diff_W1_{col}'] = df_cleanErr[\"W1mag\"] - df_cleanErr[col]\n",
    "\n",
    "# Calculate differences between W2 and each magnitude\n",
    "for col in [\"r_PStotal\", \"g_PStotal\", \"i_PStotal\", \"u_PStotal\", \"z_PStotal\"]:\n",
    "    df_colors[f'diff_W2_{col}'] = df_cleanErr[\"W2mag\"] - df_cleanErr[col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate difference between W1 and W2\n",
    "df_colors['diff_W1_W2'] = df_cleanErr['W1mag'] - df_cleanErr['W2mag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for columns in df_colors.columns:\n",
    "    print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduction dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarized the data\n",
    "X_stand = StandardScaler().fit_transform(df_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "X_train, X_val = train_test_split(X_stand, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a range of number of components to try\n",
    "n_components_range = [2, 3, 4, 5, 10, 20, 50]\n",
    "n_neighbors_range = [5, 10, 15, 20, 30, 50, 70, 100]\n",
    "\n",
    "# Initialize variables to store the best parameters\n",
    "best_silhouette_score = -1\n",
    "best_davies_bouldin_score = np.inf\n",
    "best_num_components = None\n",
    "best_n_neighbors = None\n",
    "best_labels = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over different numbers of components and neighbors\n",
    "for num_components in n_components_range:\n",
    "    for n_neighbors in n_neighbors_range:\n",
    "        # Fit UMAP model\n",
    "        reducer_ = umap.UMAP(n_neighbors=n_neighbors, n_components=num_components, random_state=42)\n",
    "        X_train_trans = reducer_.fit_transform(X_train)\n",
    "\n",
    "        # Cluster the transformed data using KMeans\n",
    "        kmeans = KMeans(n_clusters=num_components, random_state=42)\n",
    "        labels = kmeans.fit_predict(X_train_trans)\n",
    "\n",
    "        # Evaluate performance using Silhouette Score and Davies-Bouldin Index\n",
    "        silhouette = silhouette_score(X_train_trans, labels)\n",
    "        davies_bouldin = davies_bouldin_score(X_train_trans, labels)\n",
    "        print(f\"Components: {num_components}, Neighbors: {n_neighbors}, Silhouette Score: {silhouette}, DB Index: {davies_bouldin}\")\n",
    "\n",
    "        # Update best parameters based on combined metrics\n",
    "        if silhouette > best_silhouette_score and davies_bouldin < best_davies_bouldin_score:\n",
    "            best_silhouette_score = silhouette\n",
    "            best_davies_bouldin_score = davies_bouldin\n",
    "            best_num_components = num_components\n",
    "            best_n_neighbors = n_neighbors\n",
    "            best_labels = labels\n",
    "\n",
    "print(f\"Best Silhouette Score: {best_silhouette_score}\")\n",
    "print(f\"Best Davies-Bouldin Index: {best_davies_bouldin_score}\")\n",
    "print(f\"Best Number of Components: {best_num_components}\")\n",
    "print(f\"Best Number of Neighbors: {best_n_neighbors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_components = 2\n",
    "reducer = umap.UMAP(n_neighbors=50, # default 15, The size of local neighborhood (in terms of number of neighboring sample points) used for manifold approximation.\n",
    "                    n_components=num_components,  # min_samples=15, min_cluster_size=60\n",
    "                    random_state=42)\n",
    "X_trans = reducer.fit_transform(X_stand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the variance of the original data points in the reduced space\n",
    "original_variance = np.var(X_trans, axis=0)\n",
    "\n",
    "# Calculate the explained variance ratio\n",
    "explained_variance_ratio = original_variance / np.sum(original_variance)\n",
    "\n",
    "# Calculate the cumulative explained variance ratio\n",
    "cumulative_explained_variance_ratio = np.cumsum(explained_variance_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cumulative explained variance ratio\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(np.arange(1, len(cumulative_explained_variance_ratio) + 1), cumulative_explained_variance_ratio, marker='o', linestyle='-')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.title('Cumulative Explained Variance Ratio')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#umap_df = pd.DataFrame(X_trans, columns = [\"PC1\", \"PC2\", \"PC3\"])\n",
    "# Create a DataFrame for PCA results\n",
    "umap_columns = [f'PC{i}' for i in range(1, num_components + 1)]\n",
    "umap_df = pd.DataFrame(data=X_trans, columns=umap_columns)\n",
    "umap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming umap_df is your DataFrame containing UMAP components and g_PStotal column\n",
    "plt.scatter(umap_df[\"PC1\"], umap_df[\"PC2\"],\n",
    "            c=df_cleanErr[\"r_PStotal\"],  # Set color based on the values of \"g_PStotal\"\n",
    "            alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('Accent', 10))\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.colorbar(label='r_PStotal')  # Set the label of the color bar to 'g_PStotal'\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MultipleLocator, NullFormatter\n",
    "\n",
    "# Plotting\n",
    "with sns.axes_style(\"ticks\"):\n",
    "    fig, ax = plt.subplots(figsize=(15, 11))\n",
    "    plt.xlabel(\"UMAP-1\", fontsize=30)\n",
    "    plt.ylabel(\"UMAP-2\", fontsize=30)\n",
    "    plt.tick_params(axis='x', labelsize=30, width=2, length=10)  # Adjusting width of tick marks\n",
    "    plt.tick_params(axis='y', labelsize=30, width=2, length=10)  # Adjusting width of tick marks\n",
    "\n",
    "    # Create a scatter plot\n",
    "    sc = ax.scatter(umap_df[\"PC1\"], umap_df[\"PC2\"], c=df_cleanErr[\"r_PStotal\"], cmap=\"nipy_spectral\", s=100)\n",
    "    \n",
    "    # Add minor tick locators without showing the minor ticks\n",
    "    ax.xaxis.set_minor_locator(MultipleLocator(0.5))\n",
    "    ax.yaxis.set_minor_locator(MultipleLocator(0.5))\n",
    "    ax.xaxis.set_minor_formatter(NullFormatter())\n",
    "    ax.yaxis.set_minor_formatter(NullFormatter())\n",
    "\n",
    "    # Add a colorbar with improved visibility\n",
    "    cbar = plt.colorbar(sc, ax=ax, orientation='vertical', pad=0.03, format='%.1f')\n",
    "    cbar.set_label(\"Magnitude r\", fontsize=30)  # Provide a label for the colorbar\n",
    "    cbar.ax.tick_params(labelsize=30)  # Adjust the size of the tick labels\n",
    "\n",
    "#plt.savefig(\"../Figs/umap_splus_disk_wise.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = hdbscan.HDBSCAN(min_samples=5, min_cluster_size=50, \n",
    "                             prediction_data=True, gen_min_span_tree=True)\n",
    "#clusterer = hdbscan.HDBSCAN(min_cluster_size=40, min_samples=1, gen_min_span_tree=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer.fit(X_trans)\n",
    "labels = clusterer.labels_\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer.metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_cluster0 = list(labels).count(0)\n",
    "n_cluster1 = list(labels).count(1)\n",
    "n_cluster2 = list(labels).count(2)\n",
    "n_cluster3 = list(labels).count(3)\n",
    "n_noise_ = list(labels).count(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print parameters\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of cluster points 0: %d' % n_cluster0)\n",
    "print('Estimated number of cluster points 1: %d' % n_cluster1)\n",
    "print('Estimated number of cluster points 2: %d' % n_cluster2)\n",
    "print('Estimated number of cluster points 3: %d' % n_cluster3)\n",
    "print('Estimated number of noise points: %d' % n_noise_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,7))\n",
    "sns.scatterplot(x=umap_df[\"PC1\"], y=umap_df[\"PC2\"], \n",
    "                hue=labels,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenat the original claen table with the umap results\n",
    "df_cleanErr.reset_index(drop=True, inplace=True)\n",
    "umap_df.reset_index(drop=True, inplace=True)\n",
    "df_cleanErr_umap = pd.concat([df_cleanErr, umap_df], axis=1)\n",
    "df_cleanErr_umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleanErr_umap[\"Label\"] = labels\n",
    "df_cleanErr_umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data based on the \"Label\" column\n",
    "grouped_data = df_cleanErr_umap.groupby(\"Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory where you want to save the CSV files\n",
    "directory = '../Class_wise_main_unique_100neighbor/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each group\n",
    "for label, group_df in grouped_data:\n",
    "    if label == -1:  # Noise points\n",
    "        filename = f'Halpha_emitter_noise.csv'\n",
    "    else:\n",
    "        filename = f'Halpha_emitter_group{label}.csv'\n",
    "    group_df.to_csv(directory + filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
